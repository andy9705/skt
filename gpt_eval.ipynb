{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c21a6f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "import statistics\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "import re\n",
    "openai.api_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7564b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = []\n",
    "data_2 = []\n",
    "\n",
    "path_1 = \"Dialogizer\"\n",
    "path_2 = \"WikiDialog_LLM_instruction_100\"\n",
    "\n",
    "with open(path_1 + '.json','r') as f: \n",
    "    data_1 = json.load(f)\n",
    "with open(path_2 + '.json','r') as f: \n",
    "    data_2 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ec07ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant = []\n",
    "well_formed = []\n",
    "overall_quality = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29d045f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance 0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "instance 1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "instance 2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"instance {i}\")\n",
    "    context = ''\n",
    "    instance_1 = data_1[i]\n",
    "    instance_2 = data_2[i]\n",
    "    \n",
    "    if len(instance_1) != len(instance_2):\n",
    "        raise\n",
    "    \n",
    "    for j in range(len(instance_1)):\n",
    "        context += instance_1[j]['answer']\n",
    "        \n",
    "    for j in range(len(instance_1)):\n",
    "        print(j)\n",
    "        answer = instance_1[j]['answer']\n",
    "        \n",
    "        question_1 = instance_1[j]['question']\n",
    "        question_2 = instance_2[j]['question']\n",
    "        \n",
    "        PROMPT= f\"\"\"\n",
    "This is a task to evaluate the quality of a conversational question answering dataset. You\n",
    "will be given [context, two candidate questions, answer], and your task is to compare the\n",
    "quality of the candidate questions based on four criteria: contextual relevance, wellformedness, fluency, overall quality. For each criteria, answer which question is better.\n",
    "1. Contextual Relevance: whether the question relevant to the answer/context\n",
    "2. Well-formedness: whether the question is well-formed\n",
    "3. Overall Quality: overall quality of the question\n",
    "• Context: {context}\n",
    "• Question A: {question_1}\n",
    "• Question B: {question_2}\n",
    "• Answer:\n",
    "Choose the question which is more relevant to the given answer.\n",
    "options: [Question A, Equal, Question B]\n",
    "Choose the question which is more well-formed?\n",
    "options: [Question A, Equal, Question B]\n",
    "Choose the question which has better overall-quality.\n",
    "options: [Question A, Equal, Question B]\n",
    "        \"\"\"\n",
    "\n",
    "        messages = [{\"role\": \"system\", \"content\": PROMPT}]\n",
    "        response = openai.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=messages,\n",
    "                temperature=0,\n",
    "                max_tokens=150)\n",
    "        result = response.choices[0].message.content\n",
    "        \n",
    "        \n",
    "        \n",
    "        try:\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            matches = re.finditer(r'\\[', result)\n",
    "\n",
    "\n",
    "            # Find the indices of the second occurrence\n",
    "\n",
    "            first_bracket = next(matches)\n",
    "            second_bracket = next(matches)\n",
    "            third_bracket = next(matches)\n",
    "\n",
    "\n",
    "            matches = re.finditer(r'\\]', result)\n",
    "\n",
    "            # Find the indices of the second occurrence\n",
    "\n",
    "            first_bracket_end = next(matches)\n",
    "            second_bracket_end = next(matches)\n",
    "            third_bracket_end = next(matches)\n",
    "\n",
    "\n",
    "            relevant.append(result[first_bracket.start()+1:first_bracket_end.start()])\n",
    "            well_formed.append(result[second_bracket.start()+1:second_bracket_end.start()])\n",
    "            overall_quality.append(result[third_bracket.start()+1:third_bracket_end.start()])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b0178b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis is a task to evaluate the quality of a conversational question answering dataset. You\\nwill be given [context, two candidate questions, answer], and your task is to compare the\\nquality of the candidate questions based on four criteria: contextual relevance, wellformedness, fluency, overall quality. For each criteria, answer which question is better.\\n1. Contextual Relevance: whether the question relevant to the answer/context\\n2. Well-formedness: whether the question is well-formed\\n3. Overall Quality: overall quality of the question\\n• Context: A furious Paul kills her.Meanwhile, Susan and Mike make peace by having sex for the first time.Mary Alice Young (Brenda Strong) is seen in this episode.\"The 59th Street Bridge Song (Feelin\\' Groovy)\" by Harpers Bizarre and Simon & Garfunkel is played during Lynette\\'s disturbing dream.Originally the dream was to have Lynette driving through the country with her kids who were in the backseat and then proceeding to drive the car into a lake, drowning her kids, à la Susan Smith who did this very thing in South Carolina.ABC Execs were against the idea of Lynette killing her kids and Marc Cherry changed the dream to being one about suicide, thus Mary Alice was shown.\\n• Question A: Why did the ABC Execs and Marc Cherry decide to change the dream?\\n• Question B: Why was the original concept for Lynette\\'s disturbing dream changed?\\n• Answer:\\nChoose the question which is more relevant to the given answer.\\noptions: [Question A, Equal, Question B]\\nChoose the question which is more well-formed?\\noptions: [Question A, Equal, Question B]\\nChoose the question which has better overall-quality.\\noptions: [Question A, Equal, Question B]\\n        '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check single prompt\n",
    "\n",
    "PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a66c2a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Contextual Relevance: Equal\\nWell-formedness: Question B\\nOverall Quality: Question B'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check single result\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be2c1610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Equal',\n",
       " 'Question B',\n",
       " 'Question B',\n",
       " 'Equal',\n",
       " 'Equal',\n",
       " 'Question B',\n",
       " 'Question A',\n",
       " 'Equal',\n",
       " 'Question B',\n",
       " 'Equal',\n",
       " 'Equal']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "692a8c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Equal',\n",
       " 'Equal',\n",
       " 'Equal',\n",
       " 'Equal',\n",
       " 'Equal',\n",
       " 'Equal',\n",
       " 'Equal',\n",
       " 'Equal',\n",
       " 'Equal',\n",
       " 'Equal',\n",
       " 'Question B']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "well_formed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb64f509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Equal',\n",
       " 'Question B',\n",
       " 'Question B',\n",
       " 'Equal',\n",
       " 'Equal',\n",
       " 'Question B',\n",
       " 'Question A',\n",
       " 'Equal',\n",
       " 'Question B',\n",
       " 'Equal',\n",
       " 'Question B']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement",
   "language": "python",
   "name": "reinforcement"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
